{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 보배드림 크롤링\n",
    "- 문제1: 날짜 지정은 어떻게 해야할까?\n",
    "> `등록일`을 크롤링해서 날짜를 구분하자!!\n",
    "> \"/\"로 split했을 때 list len이 2이면 월일 구분, 1이면 오늘\n",
    "- 문제2: 몇 년도인지는 어떻게 구분할까?\n",
    "> 게시글 안의 날짜 크롤링!\n",
    "- 문제3: 댓글의 필요없는 앞뒤 공백들을 어떻게 없앨까?\n",
    "> `strip` 함수 사용!!\n",
    "- 문제4: 댓글 크롤링시 댓글, 대댓글이 같이 긁어져옴\n",
    "- 문제5: 댓글이 아주 많아서 댓글 페이지가 바뀌는 경우에는??(자바스크립트로 되어있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.bobaedream.co.kr/list.php?code=freeb&or_gu=10&or_se=desc&pagescale=100&s_key=호환&page=1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 게시판 url 지정\n",
    "base_url = \"http://www.bobaedream.co.kr\"\n",
    "page_url1 = \"/list.php?code=freeb&or_gu=10&or_se=desc&pagescale=100&s_key=\"\n",
    "page_url2 = \"&page=\"\n",
    "\n",
    "# 검색어 & page\n",
    "search = \"호환\"\n",
    "page_num = \"1\"\n",
    "\n",
    "url = base_url + page_url1 + search + page_url2 + page_num\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 검색어에 대한 게시글의 html 객체 가져오기\n",
    "req = requests.get(url).content\n",
    "doc = BeautifulSoup(req, \"html.parser\", from_encoding=\"utf-8\")\n",
    "# doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 게시글 url 가져오기\n",
    "article_list = doc.select(\"#boardlist > tbody > tr > td.pl14 > a.bsubject\")\n",
    "len(article_list)\n",
    "# article_url = [ i.get(\"href\") for i in article_list ]\n",
    "# article_url\n",
    "# len(article_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02/19',\n",
       " '02/16',\n",
       " '02/09',\n",
       " '02/03',\n",
       " '01/29',\n",
       " '01/28',\n",
       " '01/28',\n",
       " '01/23',\n",
       " '01/22',\n",
       " '01/19',\n",
       " '01/14',\n",
       " '01/11',\n",
       " '01/11',\n",
       " '01/08',\n",
       " '01/05',\n",
       " '12/31',\n",
       " '12/31',\n",
       " '12/30',\n",
       " '12/29',\n",
       " '12/17',\n",
       " '12/12',\n",
       " '12/10',\n",
       " '12/10',\n",
       " '12/09',\n",
       " '11/23',\n",
       " '11/22',\n",
       " '11/22',\n",
       " '11/22',\n",
       " '11/04',\n",
       " '10/01',\n",
       " '09/27',\n",
       " '09/13',\n",
       " '09/06',\n",
       " '08/22',\n",
       " '08/17',\n",
       " '07/02',\n",
       " '06/21',\n",
       " '06/16',\n",
       " '06/04',\n",
       " '05/24',\n",
       " '04/17',\n",
       " '04/06',\n",
       " '04/02',\n",
       " '02/24',\n",
       " '02/04',\n",
       " '09/10',\n",
       " '09/01',\n",
       " '08/30',\n",
       " '07/04',\n",
       " '03/31',\n",
       " '03/17',\n",
       " '03/14',\n",
       " '02/22',\n",
       " '02/14',\n",
       " '02/13',\n",
       " '01/29',\n",
       " '01/09',\n",
       " '12/26',\n",
       " '12/17',\n",
       " '12/14',\n",
       " '12/10',\n",
       " '11/26',\n",
       " '11/18',\n",
       " '11/13',\n",
       " '10/04',\n",
       " '09/09',\n",
       " '08/21',\n",
       " '07/12',\n",
       " '06/28',\n",
       " '06/18',\n",
       " '06/14',\n",
       " '05/18',\n",
       " '04/05',\n",
       " '01/25',\n",
       " '12/04',\n",
       " '12/02',\n",
       " '09/11',\n",
       " '07/15',\n",
       " '05/17',\n",
       " '04/27',\n",
       " '03/31',\n",
       " '03/30',\n",
       " '02/20',\n",
       " '01/13',\n",
       " '11/24',\n",
       " '10/27',\n",
       " '10/27',\n",
       " '10/07',\n",
       " '09/11',\n",
       " '06/28',\n",
       " '06/24',\n",
       " '05/20',\n",
       " '04/01',\n",
       " '02/18',\n",
       " '01/10',\n",
       " '01/09',\n",
       " '12/09',\n",
       " '10/24',\n",
       " '10/19',\n",
       " '09/21']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 날짜 기져오기(첫 페이지에서 best글들은 제외!!)\n",
    "date = [ i.select(\"td.date\")[0].text  for i in doc.select(\"#boardlist > tbody > tr\") if len(i.select(\"td.num01\")) == 1 ]\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 게시글 제목, 내용, 댓글 가져오기\n",
    "url_article = base_url + \"/view?code=freeb&No=1554408&bm=1\"\n",
    "req = requests.get(url_article).content\n",
    "doc = BeautifulSoup(req, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"\".join( doc.select(\"#print_area > div.writerProfile > dl > dt > strong\")[0].text.split(\"[\")[:-1] )\n",
    "content = doc.select(\"#print_area > div.content02 > div\")[0].text\n",
    "comment_list = doc.select(\"li > dl > dd\")\n",
    "comment = [ i.text.strip() for i in comment_list ]   # 댓글에 있는 앞뒤 공백 제거\n",
    "yymmdd = doc.select(\"#print_area > div.writerProfile > dl > dt > span\")[0].text.split(\"|\")[2].replace(\"\\xa0\", \" \").strip()[:10]\n",
    "yyyymm = \"\".join( yymmdd[:8].split(\".\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('나쁜 사람들이 정말 많네요',\n",
       " '\\n지겨운 월요일 잘 보내고 계신가요???저는 컴퓨터 잘 모릅니다.컴퓨터가 안켜져서 출장수리를 불렀는데 메인보드가 나가서 갈아야 한다네요출장 불렀더니 12만원 달랍니다.출장비 3만원을 쳐주더라도 9만원이 메인보드 값이라는 건데 중고에, 기존제품 돌려주지도 않고...ㅎ...그와중에 알고보니\\xa0메인보드가 기존에 쓰던거보다 하위 제품이고, 호환이 안된다고 cpu까지 하위 제품으로\\xa0바꿔놓고 1도 언급은 없고결국은 동생이 나서서 6만원 돌려받고, 메인보드, cpu 다 돌려받았습니다만좋게 얘기해서는 말이 안통하네요..거기서 바꿔준 중고\\xa0메인보드+ 중고 cpu 해도 3만원이면 될텐데..우리 메인보드 + cpu 돌려줄 생각도 없었으면서 돈도\\xa012만원 까지 받아 챙길려고 하다니양심적으로 삽시다..아무리 먹고 살기 위해서 하는 장사라지만 사람이 양심은 있어야죠...?\\n',\n",
       " '차카게 삽시다 ~ ㅎ',\n",
       " '2019.01.14')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject, content, comment[0], yymmdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 완료\n",
      "2페이지 완료\n",
      "3페이지 완료\n"
     ]
    }
   ],
   "source": [
    "# for문으로 검색어에 대한 게시글 url 가져오기\n",
    "article_url = []\n",
    "date = []\n",
    "for page in range(1,10,1):\n",
    "    # 게시판 url 지정\n",
    "    base_url = \"http://www.bobaedream.co.kr\"\n",
    "    page_url1 = \"/list.php?code=freeb&or_gu=10&or_se=desc&pagescale=100&s_key=\"\n",
    "    page_url2 = \"&page=\"\n",
    "\n",
    "    # 검색어 & page\n",
    "    search = \"호환\"\n",
    "    page_num = str(page)\n",
    "\n",
    "    url = base_url + page_url1 + search + page_url2 + page_num\n",
    "    \n",
    "    # 해당 검색어에 대한 게시글의 html 객체 가져오기\n",
    "    req = requests.get(url).content\n",
    "    doc = BeautifulSoup(req, \"html.parser\", from_encoding=\"utf-8\")\n",
    "    \n",
    "    # 게시글 url 가져오기\n",
    "    article_list = doc.select(\"#boardlist > tbody > tr > td.pl14 > a.bsubject\")\n",
    "    if len(article_list) == 0:\n",
    "        break\n",
    "    article_url = article_url + [ i.get(\"href\") for i in article_list ]\n",
    "\n",
    "    # 날짜 기져오기(best글, 관리자에 의해 삭제된 글은 제외!!)\n",
    "    date = date + [ i.select(\"td.date\")[0].text  for i in doc.select(\"#boardlist > tbody > tr\") if len(i.select(\"td.pl14 > a.bsubject\")) == 1 ]\n",
    "    \n",
    "    print(str(page) + \"페이지 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 212)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_url), len(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56개 데이터 크롤링 완료!\n"
     ]
    }
   ],
   "source": [
    "# for문으로 전체 url에 대한 글 크롤링\n",
    "subject = []\n",
    "content = []\n",
    "comment = []\n",
    "yymmdd = []\n",
    "for sub_url in article_url:\n",
    "    # 게시글 제목, 내용, 댓글 가져오기\n",
    "    url_article = base_url + sub_url\n",
    "    req = requests.get(url_article).content\n",
    "    doc = BeautifulSoup(req, \"html.parser\")\n",
    "\n",
    "    date = doc.select(\"#print_area > div.writerProfile > dl > dt > span\")[0].text.split(\"|\")[2].replace(\"\\xa0\", \" \").strip()[:10]\n",
    "    if int(date[:4]) <= 2016:\n",
    "        print(\"{}개 데이터 크롤링 완료!\".format( len(yymmdd) ))\n",
    "        break\n",
    "    else:\n",
    "        yymmdd.append(date)\n",
    "        subject.append( \"\".join( doc.select(\"#print_area > div.writerProfile > dl > dt > strong\")[0].text.split(\"[\")[:-1] ) )\n",
    "        content.append( doc.select(\"#print_area > div.content02 > div\")[0].text )\n",
    "        comment_list = doc.select(\"li > dl > dd\")\n",
    "        comment.append( [ i.text.strip() for i in comment_list ] )   # 댓글에 있는 앞뒤 공백 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 56, 56, 56)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subject), len(content), len(comment), len(yymmdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보배드림에서 검색어에 대한 게시글 url을 가져오는 함수 => date는 가져올 필요 없을듯??\n",
    "def get_url_all(search):\n",
    "    # for문으로 검색어에 대한 게시글 url 가져오기\n",
    "    article_url = []\n",
    "    date = []\n",
    "    for page in range(1, 1000, 1):\n",
    "        # 게시판 url 지정\n",
    "        base_url = \"http://www.bobaedream.co.kr\"\n",
    "        page_url1 = \"/list.php?code=freeb&or_gu=10&or_se=desc&pagescale=100&s_key=\"\n",
    "        page_url2 = \"&page=\"\n",
    "\n",
    "        # 검색어 & page\n",
    "        search = search\n",
    "        page_num = str(page)\n",
    "\n",
    "        url = base_url + page_url1 + search + page_url2 + page_num\n",
    "\n",
    "        # 해당 검색어에 대한 게시글의 html 객체 가져오기\n",
    "        req = requests.get(url).content\n",
    "        doc = BeautifulSoup(req, \"html.parser\", from_encoding=\"utf-8\")\n",
    "\n",
    "        # 게시글 url 가져오기\n",
    "        article_list = doc.select(\"#boardlist > tbody > tr > td.pl14 > a.bsubject\")\n",
    "        if len(article_list) == 0:\n",
    "            print(\"작업 완료\")\n",
    "            break\n",
    "        article_url = article_url + [ i.get(\"href\") for i in article_list ]\n",
    "\n",
    "        # 날짜 기져오기(best글, 관리자에 의해 삭제된 글은 제외!!)\n",
    "        date = date + [ i.select(\"td.date\")[0].text  for i in doc.select(\"#boardlist > tbody > tr\") if len(i.select(\"td.pl14 > a.bsubject\")) == 1 ]\n",
    "\n",
    "        print(str(page) + \"페이지 완료\")\n",
    "        \n",
    "    return article_url, date\n",
    "\n",
    "\n",
    "# 각 게시글의 제목, 내용, 댓글을 크롤링하는 함수\n",
    "def get_crawling_data(article_url):\n",
    "    # for문으로 전체 url에 대한 글 크롤링\n",
    "    subject = []\n",
    "    content = []\n",
    "    comment = []\n",
    "    yymmdd = []\n",
    "    yyyymm = []\n",
    "    for ind, sub_url in enumerate(article_url):\n",
    "        # 게시글 제목, 내용, 댓글 가져오기\n",
    "        url_article = base_url + sub_url\n",
    "        req = requests.get(url_article).content\n",
    "        doc = BeautifulSoup(req, \"html.parser\")\n",
    "        \n",
    "        date = doc.select(\"#print_area > div.writerProfile > dl > dt > span\")[0].text.split(\"|\")[2].replace(\"\\xa0\", \" \").strip()[:10]\n",
    "        if int(date[:4]) <= 2016:\n",
    "            break\n",
    "        else:\n",
    "            yymmdd.append(date)\n",
    "            yyyymm.append( \"\".join( date[:8].split(\".\") ) )\n",
    "            subject_text = doc.select(\"#print_area > div.writerProfile > dl > dt > strong\")[0].text.split(\"[\")\n",
    "            if len(subject_text) == 1:\n",
    "                subject.append(subject_text[0])\n",
    "            else:\n",
    "                subject.append( \"\".join(subject_text[:-1] ) )\n",
    "#             subject.append( \"\".join( doc.select(\"#print_area > div.writerProfile > dl > dt > strong\")[0].text.split(\"[\")[:-1] ) )\n",
    "#             subject.append( doc.select(\"#print_area > div.writerProfile > dl > dt > strong\")[0].text )\n",
    "            content.append( doc.select(\"#print_area > div.content02 > div\")[0].text )\n",
    "            comment_list = doc.select(\"li > dl > dd\")\n",
    "            comment.append( \",\".join( [ i.text.strip() for i in comment_list ]) )   # 댓글에 있는 앞뒤 공백 제거    \n",
    "        \n",
    "        # 게시글 10개마다 개수 확인\n",
    "        if (ind + 1) % 10 == 0:\n",
    "            print(\"{}개 url 완료\".format(ind + 1))\n",
    "            \n",
    "    print(\"{}개 데이터 크롤링 완료!\".format(len(yymmdd)))\n",
    "    \n",
    "    import pandas as pd\n",
    "\n",
    "    result = pd.DataFrame( {\"yyyymm\": yyyymm,\n",
    "                            \"date\": yymmdd,\n",
    "                            \"subject\": subject,\n",
    "                            \"content\": content,\n",
    "                            \"comment\": comment} )\n",
    "    \n",
    "    return subject, content, comment, yymmdd, yyyymm, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 완료\n",
      "2페이지 완료\n",
      "3페이지 완료\n",
      "작업 완료\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(211, 211)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검색어에 대한 게시글 url 가져오기\n",
    "article_url, date = get_url_all(\"호환\")\n",
    "len(article_url), len(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10개 url 완료\n",
      "20개 url 완료\n",
      "30개 url 완료\n",
      "40개 url 완료\n",
      "50개 url 완료\n",
      "55개 데이터 크롤링 완료!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55, 55, 55, 55)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 게시글에 대한 제목, 내용, 댓글 크롤링\n",
    "subject, content, comment, yymmddm, yyyymm, crawler_data = get_crawling_data(article_url)\n",
    "len(subject), len(content), len(comment), len(yyyymm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yyyymm</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>content</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201902</td>\n",
       "      <td>2019.02.19</td>\n",
       "      <td>차량 트렁크</td>\n",
       "      <td>\\r\\n제 차량이 2007년 쎄라토 차량인데 트렁크가 찌그러져서 갈아 끼울려고 하는...</td>\n",
       "      <td>될거같은데 도전!,안될것같은데유 ㄷㄷ 된다고해도 보기싫을꺼에유 ㄷㄷㄷ,만약 된다고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201902</td>\n",
       "      <td>2019.02.16</td>\n",
       "      <td>무책임한 아이코스 회사</td>\n",
       "      <td>\\r\\n안녕하세요\\r\\n너무 화가나는 일이 있어 이렇게 글을 올려요\\r\\n제가 2달...</td>\n",
       "      <td>도난 한건 본인 책임이고, 재고재품 분명있고 아이코스가 리퍼개념이라 분명 리퍼제품도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201902</td>\n",
       "      <td>2019.02.09</td>\n",
       "      <td>질문} 샤시 롤러 규격 질문 드립니다.</td>\n",
       "      <td>\\n뻘질문 죄송합니다.샤시가 오래되어 뜯어보니 롤러 수명이 다된거 같습니다.(내부 ...</td>\n",
       "      <td>엘지 하이샤시 대리점으로 문의하세요.\\n\\n시중에 물건 없습니다.,답글. 감사합니다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201902</td>\n",
       "      <td>2019.02.03</td>\n",
       "      <td>형님 동생분들 요즘  컴퓨터 어떻게들 사시는지요?!</td>\n",
       "      <td>\\r\\n컴퓨터 새로 살려고 하는데\\r\\n조립컴퓨터 조합하려고해도 각 부품호환등 이런...</td>\n",
       "      <td>그냥 인터넷 다나와,아 들어본적있어요 검색해봐야겠네요,돈주고 삽니다 ㅋㅋㅋ 아는 동...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201901</td>\n",
       "      <td>2019.01.29</td>\n",
       "      <td>Nf쏘나타 lpg차량 obd 호환</td>\n",
       "      <td>\\r\\n갑자기 차량 냉각수온 계기판이 H 끝까지 올라갑니다.\\r\\n써모스텟이랑 수온...</td>\n",
       "      <td>곤치는 방법 알려드릴까요?,싸구려 obd도 냉각수 온도는 나옵니다,계기판 얼마안하는...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   yyyymm        date                       subject  \\\n",
       "0  201902  2019.02.19                        차량 트렁크   \n",
       "1  201902  2019.02.16                  무책임한 아이코스 회사   \n",
       "2  201902  2019.02.09         질문} 샤시 롤러 규격 질문 드립니다.   \n",
       "3  201902  2019.02.03  형님 동생분들 요즘  컴퓨터 어떻게들 사시는지요?!   \n",
       "4  201901  2019.01.29            Nf쏘나타 lpg차량 obd 호환   \n",
       "\n",
       "                                             content  \\\n",
       "0  \\r\\n제 차량이 2007년 쎄라토 차량인데 트렁크가 찌그러져서 갈아 끼울려고 하는...   \n",
       "1  \\r\\n안녕하세요\\r\\n너무 화가나는 일이 있어 이렇게 글을 올려요\\r\\n제가 2달...   \n",
       "2  \\n뻘질문 죄송합니다.샤시가 오래되어 뜯어보니 롤러 수명이 다된거 같습니다.(내부 ...   \n",
       "3  \\r\\n컴퓨터 새로 살려고 하는데\\r\\n조립컴퓨터 조합하려고해도 각 부품호환등 이런...   \n",
       "4  \\r\\n갑자기 차량 냉각수온 계기판이 H 끝까지 올라갑니다.\\r\\n써모스텟이랑 수온...   \n",
       "\n",
       "                                             comment  \n",
       "0  될거같은데 도전!,안될것같은데유 ㄷㄷ 된다고해도 보기싫을꺼에유 ㄷㄷㄷ,만약 된다고 ...  \n",
       "1  도난 한건 본인 책임이고, 재고재품 분명있고 아이코스가 리퍼개념이라 분명 리퍼제품도...  \n",
       "2  엘지 하이샤시 대리점으로 문의하세요.\\n\\n시중에 물건 없습니다.,답글. 감사합니다...  \n",
       "3  그냥 인터넷 다나와,아 들어본적있어요 검색해봐야겠네요,돈주고 삽니다 ㅋㅋㅋ 아는 동...  \n",
       "4  곤치는 방법 알려드릴까요?,싸구려 obd도 냉각수 온도는 나옵니다,계기판 얼마안하는...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawler_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['차량 트렁크',\n",
       " '무책임한 아이코스 회사',\n",
       " '질문} 샤시 롤러 규격 질문 드립니다.',\n",
       " '형님 동생분들 요즘  컴퓨터 어떻게들 사시는지요?!',\n",
       " 'Nf쏘나타 lpg차량 obd 호환',\n",
       " '인터넷거래로 정품시켰는데 비슷하게생긴 싸구려가왔네요.',\n",
       " '블랙박스 sd카드  질문 좀 드립니다.',\n",
       " '조립 PC 사려는데 도움줄수 있는 분 계신가요!',\n",
       " '컴조립 고수님 사양 좀 봐주세요',\n",
       " '컴잘알 형님들 컴퓨터 견적 질문있어요..ㅠ',\n",
       " '나쁜 사람들이 정말 많네요',\n",
       " '네이게이션 SD 카드 문의좀 드리겠습니다.',\n",
       " '※※ ㅎㅂ주의!!!!!!!!!!!!! 어디서 타는 냄새 나지 않니?',\n",
       " '구형 소렌토 소모품 나눔해도 될까요',\n",
       " 'Usb로tv연결해서보는거 도움부탁드립니다',\n",
       " 'yf소나타 오디오 본체 구형 k5 오디오 패널 호환될까요?',\n",
       " '아 배고파',\n",
       " 'YF소나타 사제 내비 매립',\n",
       " '혹시 뭐가 문제 인지 아시는분... 도움을 구합니다..',\n",
       " '타이어 어디까지 호환되는지 알려주실 수 있나요?',\n",
       " 'R172 SLC200 타이어 어디까지 호환이 되나요?',\n",
       " '집에...에어컨팔려고하는데...',\n",
       " '보배회원님들 테레비문제로 궁금핟게있습니다',\n",
       " '블랙핑크는 투애니원 역대 노래 100% 호환 가능하지 않을까요',\n",
       " '벨로스터구형수동 미션과 i30 2.0 수동미션 파워트레인호환성',\n",
       " '점화 플러그 호환되는게 있나요??',\n",
       " '윈터타이어 사이즈 호환문의',\n",
       " '컴퓨터 그래픽카드/메인보드 호환',\n",
       " '단말기 호환 문의',\n",
       " 'A3에 싸구려 후방카메라 호환되나요?',\n",
       " '휠 호환 문의입니다. 알려주세요!(자동차검사관련)',\n",
       " '베라크루즈 디스크와  sm7호환',\n",
       " '블랙박스 호환 되나요??',\n",
       " '스포티지r 2륜디젤과 yf쏘나타 차량의 부품 호환여부',\n",
       " '선배님들 블랙박스도 메모리SD도 호환타나요 ?',\n",
       " '뉴오피러스와 오피러스 프리미엄 쇼바호환되나요?',\n",
       " 'tg 2.7 가솔린 lpi 점화플러그 호환되나요??',\n",
       " '블랙박스 호환? 잘아시는분 답변좀 부탁해요',\n",
       " '윈도우 스위치가 고장나서 순정부품 호환되는지 도움좀주세요',\n",
       " '아반떼 매립네비게이션 직구하려는데 호환 여부 좀 알려주세요',\n",
       " '브레이크 캘리퍼, 올뉴마티즈와 뉴스포티지 호환되나요?',\n",
       " '순정아닌 브레이크 패드 호환되는지 알 수 있는 방법 있을까요?',\n",
       " '스티어링 휠은 차종 상관없이 다 호환되나요?',\n",
       " '배그 esp 팔아요. 호환 좋아요~',\n",
       " '아반떼ad 17 인치 순정휠 호환타이어',\n",
       " '키노100cc 제네레이터, 레귤레이터 호환 가능한 대림 스쿠터 모델명 알',\n",
       " '화장실전구인데요 이게 호환이 되는게 맞을까요?',\n",
       " '블랙박스 메모리 호환되는거 좀 알려주세요.',\n",
       " 'nf소나타 로체이노베이션 머플러호환',\n",
       " '젠쿱 알미늄캘리퍼가 k7 에 호환되나요??',\n",
       " '안녕하세요 보배회원님들 bmw 335i 차량인데 아이팟 호환질문을',\n",
       " '블박별로 Sd카드 호환되는게 다른가요~?',\n",
       " '횽들 베타엔진 매니폴드 호환성좀 알려주세요~',\n",
       " '뉴모닝 2010년식인대요!구형 공조기 인대 풀오토 에어컨 공조기가 호환될',\n",
       " 'K5 일체형서스 호환질문!',\n",
       " '휴대폰 잘 아시는분들 도와주세요.(아이폰7+ LG유플러스 호환)']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yyyymm     0\n",
       "date       0\n",
       "subject    0\n",
       "content    0\n",
       "comment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawler_data.isna().sum()   # 결측치 없음!!(but comment의 경우는 값이 없는 경우가 있으나 빈칸으로 들어가있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# 웹드라이브로 크롬브라우즈 띄운다.\n",
    "driver_path = \"driver/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "driver.implicitly_wait(3)         # 암묵적으로 웹 자원 로드를 위해 3초까지 기다려 준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://kin.naver.com/search/list.nhn?&query=셧다운&period=2018.01.01.%7C2018.01.23.&page=1\n"
     ]
    }
   ],
   "source": [
    "base_url = \"http://kin.naver.com/search/list.nhn?&query=\"\n",
    "per_url_1 = \"&period=\"\n",
    "per_url_2 = \"%7C\"\n",
    "page_url = \"&page=\"\n",
    "\n",
    "query = \"셧다운\"\n",
    "per1 = \"2018.01.01.\"\n",
    "per2 = \"2018.01.23.\"\n",
    "page = str(1)\n",
    "driver.get(base_url + query + per_url_1 + per1 + per_url_2 + per2 + page_url + page)      # 네이버 지식인 1page로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://kin.naver.com/qna/detail.nhn?d1id=9&dirId=9020201&docId=292966571&qb=7IWn64uk7Jq0&enc=utf8&section=kin&rank=1&search_sort=0&spq=1',\n",
       " 'http://kin.naver.com/qna/detail.nhn?d1id=4&dirId=409&docId=292962524&qb=7IWn64uk7Jq0&enc=utf8&section=kin&rank=2&search_sort=0&spq=1',\n",
       " 'http://kin.naver.com/qna/detail.nhn?d1id=6&dirId=61302&docId=292965291&qb=7IWn64uk7Jq0&enc=utf8&section=kin&rank=3&search_sort=0&spq=1',\n",
       " 'http://kin.naver.com/qna/detail.nhn?d1id=6&dirId=614&docId=292972107&qb=7IWn64uk7Jq0&enc=utf8&section=kin&rank=4&search_sort=0&spq=1',\n",
       " 'http://kin.naver.com/qna/detail.nhn?d1id=4&dirId=40102&docId=293017325&qb=7IWn64uk7Jq0&enc=utf8&section=kin&rank=5&search_sort=0&spq=1',\n",
       " 'http://kin.naver.com/qna/detail.nhn?d1id=8&dirId=814&docId=292719046&qb=7IWn64uk7Jq0&enc=utf8&section=kin&rank=6&search_sort=0&spq=1',\n",
       " 'http://kin.naver.com/qna/detail.nhn?d1id=9&dirId=9020201&docId=293007303&qb=7IWn64uk7Jq0&enc=utf8&section=kin&rank=7&search_sort=0&spq=1',\n",
       " 'http://kin.naver.com/qna/detail.nhn?d1id=4&dirId=408&docId=292971959&qb=7IWn64uk7Jq0&enc=utf8&section=kin&rank=8&search_sort=0&spq=1',\n",
       " 'http://kin.naver.com/qna/detail.nhn?d1id=6&dirId=60217&docId=293173968&qb=7IWn64uk7Jq0&enc=utf8&section=kin&rank=9&search_sort=0&spq=1',\n",
       " 'http://kin.naver.com/qna/detail.nhn?d1id=9&dirId=9020201&docId=293168392&qb=7IWn64uk7Jq0&enc=utf8&section=kin&rank=10&search_sort=0&spq=1']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 페이지의 url 가져오기\n",
    "article_list = driver.find_elements_by_css_selector('dl > dt > a')\n",
    "article_urls = [ i.get_attribute('href') for i in article_list ]\n",
    "article_urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
