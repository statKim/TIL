{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10 MNIST and Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab10-5-mnist_nn_dropout\n",
    "### - activation fn : ReLU\n",
    "### - weight initializer : Xavier\n",
    "### - Accuracy = 0.9812 (dropout을 해서 조금 더 좋아짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-a47c5bb27b44>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train시킬 때 몇퍼센트의 node들만 포함시켜서 할 지 정하는 값\n",
    "# 나중에 test할 때는 반드시 1로 해주고 해야함!!!\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-f02e71557745>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.451240691\n",
      "Epoch: 0002 cost = 0.174481454\n",
      "Epoch: 0003 cost = 0.128587236\n",
      "Epoch: 0004 cost = 0.109119285\n",
      "Epoch: 0005 cost = 0.092670795\n",
      "Epoch: 0006 cost = 0.082088990\n",
      "Epoch: 0007 cost = 0.077748027\n",
      "Epoch: 0008 cost = 0.070568659\n",
      "Epoch: 0009 cost = 0.063686291\n",
      "Epoch: 0010 cost = 0.057611010\n",
      "Epoch: 0011 cost = 0.062052387\n",
      "Epoch: 0012 cost = 0.052562773\n",
      "Epoch: 0013 cost = 0.051294356\n",
      "Epoch: 0014 cost = 0.046317724\n",
      "Epoch: 0015 cost = 0.047256546\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}  # keep_prob:0.7 => 70%만을 이용해서 train하겠다!!\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9812\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))   # test할 때에는 모든 node를 이용!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    }
   ],
   "source": [
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIQfCOr/AXJUIgRS2v0mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x242bb53aa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable W1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-50a465d5873c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m W1 = tf.get_variable(\"W1\", shape=[784, 512],\n\u001b[1;32m---> 55\u001b[1;33m                      initializer=tf.contrib.layers.xavier_initializer())\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mL1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1465\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1467\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1215\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    525\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    479\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[1;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    846\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 848\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable W1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "Epoch: 0001 cost = 0.447322626\n",
    "Epoch: 0002 cost = 0.157285590\n",
    "Epoch: 0003 cost = 0.121884535\n",
    "Epoch: 0004 cost = 0.098128681\n",
    "Epoch: 0005 cost = 0.082901778\n",
    "Epoch: 0006 cost = 0.075337573\n",
    "Epoch: 0007 cost = 0.069752543\n",
    "Epoch: 0008 cost = 0.060884363\n",
    "Epoch: 0009 cost = 0.055276413\n",
    "Epoch: 0010 cost = 0.054631256\n",
    "Epoch: 0011 cost = 0.049675195\n",
    "Epoch: 0012 cost = 0.049125314\n",
    "Epoch: 0013 cost = 0.047231930\n",
    "Epoch: 0014 cost = 0.041290121\n",
    "Epoch: 0015 cost = 0.043621063\n",
    "Learning Finished!\n",
    "Accuracy: 0.9804\n",
    "'''\n",
    "\n",
    "# Accuracy = 0.9812     # dropout을 해서 조금 더 좋아짐\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# train시킬 때 몇퍼센트의 node들만 포함시켜서 할 지 정하는 값\n",
    "# 나중에 test할 때는 반드시 1로 해주고 해야함!!!\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}  # keep_prob:0.7 => 70%만을 이용해서 train하겠다!!\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))   # test할 때에는 모든 node를 이용!!!\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "'''\n",
    "Epoch: 0001 cost = 0.447322626\n",
    "Epoch: 0002 cost = 0.157285590\n",
    "Epoch: 0003 cost = 0.121884535\n",
    "Epoch: 0004 cost = 0.098128681\n",
    "Epoch: 0005 cost = 0.082901778\n",
    "Epoch: 0006 cost = 0.075337573\n",
    "Epoch: 0007 cost = 0.069752543\n",
    "Epoch: 0008 cost = 0.060884363\n",
    "Epoch: 0009 cost = 0.055276413\n",
    "Epoch: 0010 cost = 0.054631256\n",
    "Epoch: 0011 cost = 0.049675195\n",
    "Epoch: 0012 cost = 0.049125314\n",
    "Epoch: 0013 cost = 0.047231930\n",
    "Epoch: 0014 cost = 0.041290121\n",
    "Epoch: 0015 cost = 0.043621063\n",
    "Learning Finished!\n",
    "Accuracy: 0.9804\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
